## ID3
    1.采用信息增益
        信息增益越大越好
    2.构建的是多叉树
        每次划分出多个分支（计算每个特征的信息增益），直到叶子的样本中类别相同或没有特征可分
        PS.每个特征只用用于一次分支，分支后就删除该特征
    3.不支持连续值和剪枝

## C4.5
    1.采用信息增益率
        
    2.构建的是多叉树
        
    3.支持连续值和剪枝

## CART分类树

    1.采用Gini指数
        Gini指数越小越好
    2.构建的是二叉树
        一个特征可能多次用于分支，每次分支划分出两个分支（计算各个特征的各个值的Gini指数），直到样本小于样本阈值或Gini指数小于Gini指数阈值。
        PS.特征可反复使用，用过特征分支后不会删除特征
    3.支持连续值和剪枝


## CART回归树
    1.思想：当特征划分的足够细时，可以近似的用分类的样本均值作为预测值。
    2.采用误差平方和度量
        计算每个特征的每个属性分支后的MSE，取最小MSE对应的特征-特征值作为划分点。
    3.最终预测值
        最终的叶子输出的预测值是取叶子中的样本均值作为预测值

    


## 处理和优化

1.连续数据离散化


2.预剪枝和后剪枝
    代价复杂性剪枝、最小误差剪枝、悲观误差剪枝等等。